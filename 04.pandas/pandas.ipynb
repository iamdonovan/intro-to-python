{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d988b6-16ca-44ad-a71e-0d45faa2b50d",
   "metadata": {},
   "source": [
    "# working with pandas dataframes\n",
    "\n",
    "In this lesson, we're going to learn how we can work with datasets - combining tables, creating and re-arranging variables, selecting and sorting rows, and grouping and summarizing data. Mostly, we will be working with `pandas`, which is designed for data manipulation (again, read this as \"analyzing\" or \"working with\", not \"fabricating\"!).\n",
    "\n",
    "## data\n",
    "\n",
    "The data used in this exercise are the historic meteorological observations from the [Armagh Observatory](https://www.metoffice.gov.uk/weather/learn-about/how-forecasts-are-made/observations/recording-observations-for-over-100-years) (1853-present), the Oxford Observatory (1853-present), the Southampton Observatory (1855-2000), and Stornoway Airport (1873-present), downloaded from the [UK Met Office](https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data).\n",
    "\n",
    "Like with the Armagh dataset we used previously, I have done the following to make the data slightly easier to work with:\n",
    "- Removed the header on lines 1-5\n",
    "- Replaced multiple spaces with a single space, and replaced single spaces with a comma (`,`)\n",
    "- Removed `---` to indicate no data, leaving these fields blank\n",
    "- Removed `*` indicating provisional/estimated values\n",
    "- Removed the 2023 data\n",
    "- Renamed the file (e.g., `oxforddata.txt` -> `oxforddata.csv`).\n",
    "\n",
    "If you wish to use your own data (and there are loads of stations available!), please feel free. I have also included a script, `convert_metoffice.py` (in the `scripts/` folder), that will do these steps automatically. All you need to do is run the following from a terminal:\n",
    "\n",
    "```\n",
    "    convert_metoffice.py {station}data.txt\n",
    "\n",
    "```\n",
    "\n",
    "This will create a new file, `{station}data.csv`, that has converted + cleaned the data into a CSV format that can easily be read by `pandas`.\n",
    "\n",
    "## loading libraries\n",
    "\n",
    "As before, we load the libraries that we will use in the exercise at the beginning. We will be using three libraries:\n",
    "- [pandas](https://pandas.pydata.org/), for reading the data from a file;\n",
    "- [seaborn](https://seaborn.pydata.org/), for plotting the data;\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html), for working with filesystem paths;\n",
    "\n",
    "As before, we're going to *alias* `pandas` as `pd` and `seaborn` as `sns`. We only want the `Path` **sub-package** from `pathlib`, so we use `from` to specify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13106b-1bcf-4f59-9c72-ed011278586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09560ac-12f4-44a2-bac2-84c05e36d8b8",
   "metadata": {},
   "source": [
    "In this exercise, we're going to see a number of different ways that we can work with data tables. Before we get to this, however, we need to load the individual data files and combine them into a single data table.\n",
    "\n",
    "Rather than loading all four at once and then combining them, however, we can simplify this slightly using a **for** loop. First, we'll load the Armagh Observatory data, because it's currently in a different folder.\n",
    "\n",
    "Rather than typing the path to the file directly, we can use `pathlib.Path` ([documentation](https://docs.python.org/3/library/pathlib.html#pathlib.Path)) to construct a path in a *platform-independent* way. In general, we want to do this because Windows uses `\\` to separate folders, while Unix-style systems such as Linux and MacOS use `/` - this way, we don't run into issues if we share our code with people working on different systems.\n",
    "\n",
    "To construct the filename, we're using a [*relative* path](https://en.wikipedia.org/wiki/Path_(computing)#Absolute_and_relative_paths) - that is, it is *relative* to some given working directory (typically the current working directory). To get to **armaghdata.csv** from the current directory, we have to go up a directory level (`..`), before entering the **03.plotting** directory, and the **data** directory after that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f041dfe-5263-42ff-990f-b5fefd0d12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv(Path('..', '03.plotting', 'data', 'armaghdata.csv')) # use file.path to construct a path to the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ddcd5-9ca7-4786-a329-ddcef20c9f85",
   "metadata": {},
   "source": [
    "Next, we want to make sure that we can keep track of which observation comes from which station - so, we should add a `station` variable to the table, and make sure to specify that these observations all come from the Armagh Observatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3a2a9-a76a-4b41-8927-15c1f298eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['station'] = 'armagh' # add the station name as a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03590e26-22c3-4b5d-861c-33e3c325b876",
   "metadata": {},
   "source": [
    "Now, we can set up a loop to load the other 3 stations data. First, we can create a **list** of station names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1215aa-0819-4066-8710-762fe94e55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of station names\n",
    "new_stations = ['oxford', 'southampton', 'stornoway']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf965731-9f8f-46f9-a183-a95e423bd914",
   "metadata": {},
   "source": [
    "Now that we have the vector of station names, we can construct the **for** loop to first read each file:\n",
    "\n",
    "```python\n",
    "    fn_data = Path('data', f\"{station}data.csv\")\n",
    "\n",
    "```\n",
    "\n",
    "Here, we use an **f-string** to combine the `station` variable (which takes on a value from the `new_stations` **list** on each pass through the loop) with `'data.csv'`, so that the resulting file names will be `'oxforddata.csv'`, `'southamptondata.csv'`, and `'stornowaydata.csv'`. We then use `Path()` to combine this with the `'data'` directory name, so that the value of `fn_data` is the complete relative path to each file.\n",
    "\n",
    "Next, we again use `pd.read_csv()` to read in the file, and add a `station` variable to the table, just like we did with the Armagh data. \n",
    "\n",
    "Finally, we use `pd.concat()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)) to combine the existing table, `station_data`, with the newly loaded table (`data`), and overwrite the value of `station_data` with this combined table:\n",
    "\n",
    "```python\n",
    "    station_data = pd.concat([station_data, data], ignore_index=True)\n",
    "\n",
    "```\n",
    "\n",
    "Remember that each time through the **for** loop, the value of `station` is updated - so on the first time through, the value of `station` will be `'oxford'`, on the second time through it will be `'southampton'`, and on the final time through it will be `'stornoway'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02e0c9-b1f2-4e57-860a-1462a9114b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in new_stations:\n",
    "    fn_data = Path('data', f\"{station}data.csv\") # create the filename for each csv file, using file.path and paste\n",
    "    data = pd.read_csv(fn_data) # read the csv\n",
    "    data['station'] = station # add the station to the table\n",
    "\n",
    "    station_data = pd.concat([station_data, data], ignore_index=True) # combine the new data with the current data table\n",
    "\n",
    "print(station_data) # show the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325becd3-fb17-48c4-9314-59f225aa8239",
   "metadata": {},
   "source": [
    "Note that this is one advantage of using **clear, consistent naming and formatting for data files** - it means that we can easily write a loop to load multiple files, instead of having to write individual paths or treat each file differently!\n",
    "\n",
    "## selecting rows using expressions\n",
    "\n",
    "Now that we have a single table, we can also look at ways that we can select rows from the table. For a *very* in-depth overview of how indexing and slicing works with `pandas`, see this [guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html).\n",
    "\n",
    "We have alread seen an example of this - for example, we could select all observations where the monthly maximum temperature (`tmax`) is greater than 20°C by using `.loc` and a conditional statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8685b-871f-402d-8631-5a06fbe4e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.loc[station_data['tmax'] > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff430bcf-9e94-4fb6-9931-6b1fde1ef64e",
   "metadata": {},
   "source": [
    "Remember that if we want to use multiple conditions - for example, all observations where the monthly maximum temperature is greater than 20°C, and the monthly rainfall is greater than 100 mm, we can't simply use the `&` operator with the two statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb3bf8-6fd0-4b4e-99ce-f0b337b60c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_data.loc[station_data['tmax'] > 20 & station_data['rain'] > 100] # this won't work to combine conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894338f1-6c80-49e4-b7d7-d89e781bb055",
   "metadata": {},
   "source": [
    "Instead, we have to surround each condition with parentheses first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6430c92-5fcf-4d92-b289-3f8fe1528bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_data.loc[(station_data['tmax'] > 20) & (station_data['rain'] > 100)] # this will work to select tmax > 20 and rain > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec935af-d5c6-46a0-a0b0-5a3ddf93299d",
   "metadata": {},
   "source": [
    "Alternatively, we can also use the `.query()` method ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html)), which allows us to write slightly more natural expressions. The selection above using `.query()` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67badbf6-b582-4823-bf30-b3056c67159e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_data.query('tmax > 20 & rain > 100') # use query to select rows where tmax > 20 and rain > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e599284-3ccf-430f-b4c1-71333252589a",
   "metadata": {},
   "source": [
    "We can also use variables in the query - we just need to prefix them with `@`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d4ee6-36df-4992-b5d9-fde28760f5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_temp = 20 # create a new variable with a value of 20\n",
    "\n",
    "station_data.query('tmax > @min_temp & rain > 100') # reference the new variable in the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a0cdf3-a8ed-456c-b109-5618e5350ab0",
   "metadata": {},
   "source": [
    "## using sort_values to sort rows\n",
    "\n",
    "Sometimes, we might want to sort our data according to the value of different variables. For example, we can sort the observations by rainfall, from smallest to largest values, using `.sort_values()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3dd03-8ca9-4043-8e4f-cb63d0bb7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.sort_values('rain') # sort by rainfall, from smallest to largest values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43ffb2-8ae9-4d1a-ab5f-e20877f9436d",
   "metadata": {},
   "source": [
    "By default, the values are sorted in *ascending* order (from smallest to largest, or from A to Z for characters). If we want to see the reverse, we can set the `ascending` keyword argument to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c551b-c873-42f6-bfbd-0da06fe35e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.sort_values('rain', ascending=False) # sort by rainfall, from largest to smallest values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d4651-4420-4257-94ba-376e22e226a4",
   "metadata": {},
   "source": [
    "Note that in both cases, `NaN` values come at the bottom - because they are not numbers, they are not sorted as being greater than or less than other values, so `pandas` moves them to the end by default (to put them at the beginning, we can use the `na_position` argument).\n",
    "\n",
    "## find unique values\n",
    "\n",
    "To find unique rows in a **Series** (column), we can use `.unique()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html)). For example, we can find the unique values of the `station` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cb22c-f16a-4fc4-b16c-21bf11ea2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['station'].unique() # find unique values of station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a2449-068b-4906-ad11-8f852f6e1b45",
   "metadata": {},
   "source": [
    "We can also `.drop_duplicates()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)) to find only unique rows in a **DataFrame**. With the `subset` argument, we can choose which columns to use in determining whether rows are unique/duplicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da10a7-ef2e-4975-b3eb-423640fba97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.drop_duplicates(subset='station') # find rows based on unique values of station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a4f66-cbb5-4972-868d-e675ef2b1a7f",
   "metadata": {},
   "source": [
    "We can also use it to find combinations of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53257c0-de7d-4266-a55d-1b5c76ee0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.drop_duplicates(subset=['station', 'mm']) # find rows with unique station/month pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8340e-9f84-40ee-b6b1-1c3de940b4f2",
   "metadata": {},
   "source": [
    "Note that the distinct values found above are all from the first year of each dataset - this is because `.drop_duplicates()` discards all but the first occurrence of a unique row.\n",
    "\n",
    "## counting occurrences\n",
    "\n",
    "If we want to count the number of non-NaN values in a table, we can use `.count()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242260b-b1e7-4c54-af23-5589d247bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.count() # count the number of non-nan values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6937a1-661f-4844-ab25-38273cf248f1",
   "metadata": {},
   "source": [
    "If we want to find the frequency of each distinct row in a **DataFrame**, we can use `.value_counts()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html)). By itself, this looks at all columns in a row to determine whether or not the row is unique. More often, we will probably want to specify which columns to use with the `subset` argument.\n",
    "\n",
    "For example, we can count the number of times each station observed rainfall greater than 150 mm in a month by first using `query()` to select all rows where `rain` is greater than 150, then use `value_counts()` with the `subset` argument to count the number of unique occurrences of `station` in the resulting table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01466ed2-9d8e-472f-9e19-c24eeb8eed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .query('rain > 150') \\\n",
    "    .value_counts(subset='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58768397-56e1-49c0-8267-f7612e1c84ae",
   "metadata": {},
   "source": [
    "From this, we can quickly see that Stornoway Airport, located in the Outer Hebrides, has far more months with heavy rainfall (278) than any other station in our dataset; by contrast, Oxford has only recorded 12 such months between 1853 and 2022.\n",
    "\n",
    "Note that in this cell, we're also using the **line break** character, `\\`, to split the call across multiple lines to help with readability. As far as python is concerned, there is no difference between this:\n",
    "\n",
    "```python\n",
    "station_data \\\n",
    "    .query('rain > 150') \\\n",
    "    .value_counts(subset='station')\n",
    "```\n",
    "\n",
    "and this:\n",
    "\n",
    "```python\n",
    "station_data.query('rain > 150').value_counts(subset='station')\n",
    "\n",
    "```\n",
    "\n",
    "But, the former can be easier to read/understand what is being done. You will likely see code written in both styles, but I will try to break things into different lines when it makes sense.\n",
    "\n",
    "## adding columns to the table\n",
    "\n",
    "In a previous exercise, we saw how we can add a variable/column to a **DataFrame** using the output of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4254a-cfed-47aa-96cb-9f2e7e423133",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['date'] = pd.to_datetime({'year': station_data['yyyy'], 'month': station_data['mm'], 'day': 1}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6567ad1-04cf-4620-aebf-f094b0962106",
   "metadata": {},
   "source": [
    "And, we saw how we could assign values to a column based on the values in other columns:\n",
    "\n",
    "```python\n",
    "station_data['season'] = '' # initialize an empty string column\n",
    "station_data.loc[station_data['mm'].isin([1, 2, 12]), 'season'] = 'winter' # if month is 1, 2, or 12, set season to winter\n",
    "station_data.loc[station_data['mm'].isin(range(3, 6)), 'season'] = 'spring' # if month is 3, 4, or 5, set season to spring\n",
    "station_data.loc[station_data['mm'].isin(range(6, 9)), 'season'] = 'summer' # if month is 6, 7, or 8, set season to summer\n",
    "station_data.loc[station_data['mm'].isin(range(9, 12)), 'season'] = 'autumn' # if month is 9, 10, or 11, set season to autumn\n",
    "\n",
    "```\n",
    "\n",
    "Now, let's look at another way that we can accomplish the same thing, in a slightly more \"[pythonic](https://stackoverflow.com/a/25011492)\" way, by using some of the features of the language.\n",
    "\n",
    "First, we'll use `range()` ([documentation](https://docs.python.org/3/library/functions.html#func-range)) to get a list of numbers from 1 to 12, corresponding to the months of the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba73ca-8b03-496a-8004-cdc7e591f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = range(1, 13) # get a list of numbers from 1 to 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411d86-2ab3-4c15-8200-f1f7da8ebd27",
   "metadata": {},
   "source": [
    "Next, we'll use list multiplication and addition to create a list of the season names for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8582c5-2c34-4ee6-8383-bce99b216822",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['winter'] * 2 + ['spring'] * 3 + ['summer'] * 3 + ['autumn'] * 3 + ['winter']\n",
    "\n",
    "seasons # show the list of season names for each month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9954da-564d-46e5-af12-cb649cb3bd36",
   "metadata": {},
   "source": [
    "We could, of course, have written this out explicitly:\n",
    "\n",
    "```python\n",
    "seasons = ['winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'autumn', 'autumn', 'autumn', 'winter']\n",
    "\n",
    "```\n",
    "\n",
    "Instead, we have used the fact that multiplying a **list** by an integer repeats the **list**, and adding **list**s together *concatenates* them, to simplify this (and also to remind you of these properties of **list**s).\n",
    "\n",
    "Next, we can create a **dict()** using `zip()` ([documentation](https://docs.python.org/3/library/functions.html#zip)) to create pairs of month number/season name values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659075e-d355-462e-9fe6-dea032a3478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(months, seasons)) # create a dict() of month/season pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f72881-221f-4bbe-843a-ae52e8be4f15",
   "metadata": {},
   "source": [
    "Finally, we will use `.map()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)) to assign season names to each row, based on the value of `mm` (the month number):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c066f6e-daed-414f-a06e-77c0adcf19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['season'] = station_data['mm'].map(dict(zip(months, seasons)))\n",
    "\n",
    "station_data.head(n=12) # show the first 12 rows of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d755546-a65f-486b-a7cd-41acf381030c",
   "metadata": {},
   "source": [
    "## re-naming columns\n",
    "\n",
    "Often, we may also want to rename variables to make them easier to read/understand. For example, the `yyyy`, `mm`, and `af` variables in our table are not necessarily the easiest to understand. We can rename them to more clear names, such as `year`, `month`, and `air_frost`, using the `.rename()` method ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)). \n",
    "\n",
    "To make it clear that we are renaming the columns, we'll use the `columns` argument, passing a `dict()` of old/new names. We also want this change to happen \"in place\", meaning that it should update the column names of the existing **DataFrame**, rather than returning a new **DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66402b-00a3-4d8e-a90d-a8c61a6cba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.rename(columns={'yyyy': 'year', 'mm': 'month', 'af': 'air_frost'}, # rename columns using old/new name pairs\n",
    "                    inplace=True # update the values of this dataframe, not return a new one\n",
    "                   )\n",
    "\n",
    "station_data.head(n=5) # show the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3948d-70e6-47e8-ad35-aaa82a7b4ffe",
   "metadata": {},
   "source": [
    "Many of the methods that we are working with in this exercise have an `inplace` argument - by default, `pandas` assumes that you don't want to overwrite the existing **DataFrame** object with these changes. If we don't use the `inplace` argument, we need to assign the output to a new variable in order to use it; for example:\n",
    "\n",
    "```python\n",
    "new_df = old_df.rename(columns={'old_name': 'new_name'})\n",
    "\n",
    "```\n",
    "\n",
    "## selecting columns\n",
    "\n",
    "Selecting columns from a **DataFrame** works similarly to selecting rows. We can use square brackets (`[` and `]`) along with the name of the column (as a **str**ing) to select a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfe549-cea0-49f2-bb11-1af0a5edb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['rain'] # select the rain column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1ed16-9f90-493f-a976-e18ddc256738",
   "metadata": {},
   "source": [
    "If we want to select multiple columns, we can use a **list** of column names inside of the square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f497e3-e8ec-41eb-930d-a161a6d17893",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data[['date', 'rain', 'station', 'season']] # select the date, rain, station, and season columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192237c-5437-4834-9ef2-5735e97a765d",
   "metadata": {},
   "source": [
    "Note that the order of the output will be the same as the order of the input - so, this is one way that we can also re-arrange columns.\n",
    "\n",
    "You can also select a *slice* of columns using the `:` operator. Note that unlike how we have seen this used before, when used to select columns (or rows) from a **DataFrame** using labels, `:` is inclusive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230b214-2e00-403a-a178-a4b33c274ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.loc[:, 'year':'rain'] # select all columns from year to rain (inclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471779b8-2823-4777-a6ab-6cbbde8bdf54",
   "metadata": {},
   "source": [
    "Finally, we can also use `.filter()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html)) to select columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b951636-ea1d-43ef-8f50-fe9097403d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.filter(['date', 'rain']) # select the date and rain columns using filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9440c2-777d-49ac-810b-df850db66da2",
   "metadata": {},
   "source": [
    "## re-arranging columns using reindex()\n",
    "\n",
    "We might also want to re-arrange the order of columns - there are a number of different ways to do this, but we'll have a look at using `.reindex()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html)). \n",
    "\n",
    "First, we can create a **list** of the column names, in the order we want to see them. We then pass this to `.reindex()`, using the `columns` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4774b58-aab8-409a-8068-3778151f7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['date', 'year', 'month', 'season', 'station', 'tmax', 'tmin', 'air_frost', 'rain', 'sun']\n",
    "station_data = station_data.reindex(columns=new_order) # change the order of the columns and assign the output to the same variable\n",
    "\n",
    "station_data.head() # show the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4704fd-8cbc-4ca1-a658-b32a3c152fab",
   "metadata": {},
   "source": [
    "## saving data to a file\n",
    "\n",
    "Now that we have combined the different data files, added some new columns to our data, and re-named and re-arranged the columns, we should save our dataset to a file. We'll use the `.to_csv()` method ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)) to do this. As with reading files, though, there are other file formatting options. \n",
    "\n",
    "Once again, we will use `Path` to create a path object to write the file to; we also set the `index` argument to `False` so that `pandas` doesn't write the row numbers to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de89b9-677d-46c3-bb98-c5f60d5887a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.to_csv(Path('data', 'combined_stations.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266173d4-6201-48ab-a0ea-52d5fd6de480",
   "metadata": {},
   "source": [
    "Now, we'll be able to load this file when we want to do further analysis, rather than needing to re-run the steps to load each file, combine the tables, create new variables, and so on.\n",
    "\n",
    "## grouping data\n",
    "\n",
    "Next, we'll see how we can use different tools to aggregate and summarize our data, starting with `.groupby()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)). To start, we'll group the data by `station`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985544e-481f-49ac-8815-3685720472cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.groupby('station') # group the data by station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a47ac6-c38c-490e-bf54-aaf5b592515e",
   "metadata": {},
   "source": [
    "Here, we don't see anything special - just that the output of `.groupby()` is, by itself, a **DataFrameGroupBy** object. Among other things, though, we can use this object to calculate [descriptive statistics](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html#dataframegroupby-computations-descriptive-stats) for each column, based on the applied groupings. \n",
    "\n",
    "For example, we can use `.mean()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.mean.html)) to calculate the mean value of each column (specifying `numeric_only=True` to make sure that we only get a result for numeric columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfde32d-39ac-4dee-8bfa-91d5453ca498",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .groupby('station') \\\n",
    "    .mean(numeric_only=True) # specify numeric_only=True to avoid warning messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ef880-2c83-4142-8b10-3906ec1d5743",
   "metadata": {},
   "source": [
    "Note that by default, `.groupby()` drops `NaN` values - if we want to keep these, we need to specify `dropna=False` when we create the groupings.\n",
    "\n",
    "Now, let's combine this with what we learned in the previous lesson (the plotting exercise) to create a plot that shows the distribution of rainfall by season, separated by station. First, we want to create a plot that shows the density distribution of rainfall for each season, using `sns.FacetGrid()` to create a single panel for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bec4cc-17e6-49af-af6c-48afbebb2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=station_data, col='station', hue='season', col_wrap=2) # create a 2x2 grid with a panel for each station\n",
    "g.map_dataframe(sns.kdeplot, x='rain', fill=True) # plot the density of rainfall\n",
    "g.add_legend() # add a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a6be8-aad0-4c43-8bfb-9ed882ecde09",
   "metadata": {},
   "source": [
    "Next, we can use `group_by()` to calculate the mean rainfall for each station, and assign this to a new variable, `mean_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d6782-590b-4c48-aeec-8e574bc2b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = station_data.groupby('station')['rain'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d448e-4d0f-48e3-bfd0-6efb5fca0547",
   "metadata": {},
   "source": [
    "Now, we'll iterate over axis and mean value pairs to plot a vertical line using `matplotlib.pyplot.axvline()` ([documentation](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.axvline.html)). As we mentioned in the previous exercise, `seaborn`, like many other plotting packages, is built on top of `matplotlib` - meaning that many `seaborn` objects inherit from correpsonding `matplotlib` objects.\n",
    "\n",
    "First, though, we'll make sure that we're plotting in the correct panel by using the `axes_dict` *attribute* of our **FacetGrid**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1d5e6-8094-42d2-8034-1c2d98774e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.axes_dict # show the dict of key/value pairs for the facetgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a3b0e-0a22-4daa-8eed-5e8b08492023",
   "metadata": {},
   "source": [
    "We can iterate over the `index` of `mean_values` (which corresponds to each station), then use the `axes_dict` to plot a vertical line corresponding to each mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d11a9-6b09-488a-8426-aae7994a8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in mean_values.index: # iterate over station names\n",
    "    g.axes_dict[station].axvline(x=mean_values[station], color='k', linestyle='--') # plot a vertical line at the mean rain value for each station\n",
    "\n",
    "g.fig # show the updated figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d77f5d-1237-4b79-b0d4-bc1620b8a22c",
   "metadata": {},
   "source": [
    "In the next panel, write some lines of code to change the axes labels and increase the font size for the tick labels, axis labels, and panel labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf227bb-bfe7-4490-b660-f5a0a8c86adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1aa37a-c127-47de-b159-d14a200acfc7",
   "metadata": {},
   "source": [
    "Now that you have finished the plot, be sure to save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f1c5f-af67-4414-8719-92ecee5f12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.fig.savefig('seasonal_rain_distribution.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7495a4-7ef6-479b-a294-bf5bf5e2bce6",
   "metadata": {},
   "source": [
    "## slicing\n",
    "\n",
    "We'll finish up by looking at a few functions that we can use to *slice* a dataset - that is, extract specific rows from a group. For example, we can use `.loc` along with the \n",
    "`.idxmax()` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmax.html)) to get the row corresponding to the maximum value of `rain` (for the minimum, we would use `.idxmin()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24aa39a-72c9-47d6-be99-d1555ea96dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_data.loc[station_data['rain'].idxmax()] # use idxmax to find the index of the maxmimum value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2903e-7677-4a63-9363-b9e89ca8177d",
   "metadata": {},
   "source": [
    "We can also make use of `.head()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)), along with `.sort_values()`, to select `n` rows corresponding to the maximum value of one or more variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276dcbd-8082-4ff6-84c1-729b9dd4dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .sort_values('rain', ascending=False) \\\n",
    "    .head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680da35-6cef-43c9-aee4-b07c40618dbc",
   "metadata": {},
   "source": [
    "Alternatively, we can use `.tail()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html)), which returns the last `n` rows of the **DataFrame** (note, however, that this may give us `NaN` values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fad05c-4e4b-43d7-abaa-5335b2bf941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .sort_values('rain') \\\n",
    "    .tail(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bf7fa-1169-4d3c-911a-97a85a013cc3",
   "metadata": {},
   "source": [
    "Let's say that we wanted to find the month with the most rain from each of the stations. To do this, we can first sort `rain` in descending order, then group based on `station`, before using `.head()` to select the first row for each value of `station`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149c7ea-eaf9-4822-a938-aab20626aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .sort_values('rain', ascending=False) \\\n",
    "    .groupby('station') \\\n",
    "    .head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ed426-fbca-4301-84a3-904029083a44",
   "metadata": {},
   "source": [
    "Finally, we can select a random sample from a **DataFrame** using `.sample()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html)). On a grouped **DataFrame**, we get a random sample from each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e9581-f6b7-45d7-b053-76575ad40521",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data \\\n",
    "    .groupby('station') \\\n",
    "    .sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc92be-2a19-48d0-a0b8-ae3ff3c72453",
   "metadata": {},
   "source": [
    "## exercise and next steps\n",
    "\n",
    "That's all for this exercise. To practice your skills, create a notebook file that does the following:\n",
    "\n",
    "- loads the libraries that you need\n",
    "- loads the saved data file (**combined_stations.csv**)\n",
    "- helps you answer the following questions:\n",
    "    - what station has the highest recorded rainfall in the past 20 years, and on what date?\n",
    "    - what season has the lowest average rainfall? is it the same season for all four stations?\n",
    "    - what station has recorded the most months with `tmin` < 1°C? are all these observations from a single season?\n",
    "    - what is the median rainfall in months where `tmax` is greater than 20°C? make sure that your result is a number, not a **DataFrame**!\n",
    "    - what year saw the most total rainfall, using data from all four stations?\n",
    "    - what are the top 5 driest years, using only data from stations in Britain?\n",
    "    - what is the lowest annually-averaged monthly minimum temperature in the dataset, as measured by a single station?\n",
    "    - what is the sunniest month, on average, in Armagh?\n",
    "        - bonus: write a line that will rename the months from the number to a 3-letter abbreviation (**hint**: we saw an example of this using `.map()`)\n",
    "\n",
    "For a bonus, try downloading at least one additional dataset from the [Met Office](https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data), saving it to the **data** folder, and using the script provided (`convert_metoffice.py`) to convert the `.txt` file into a `.csv` file. \n",
    "\n",
    "In your new notebook file, remember to add this new data to your existing dataset (and re-save the file!), then repeat the analyis questions above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
